{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST data\n",
    "\"\"\"\n",
    "Load the MNIST dataset into numpy arrays\n",
    "Author: Alexandre Drouin\n",
    "License: BSD\n",
    "\"\"\"\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST data/\", one_hot=True)\n",
    "X_train = np.vstack([img.reshape((28, 28)) for img in mnist.train.images])\n",
    "Y_train = mnist.train.labels\n",
    "X_test = np.vstack([img.reshape(28, 28) for img in mnist.test.images])\n",
    "Y_test = mnist.test.labels\n",
    "del mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the data shape\n",
    "\n",
    "print(\"Shape of x train\",X_train.shape)\n",
    "print(\"Shape of y test\",Y_train.shape)\n",
    "print(\"Shape of x train\",X_test.shape)\n",
    "print(\"Shape of y test\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the data\n",
    "\n",
    "X_train = np.reshape(X_train,(-1,784))\n",
    "Y_train = np.reshape(Y_train,(-1,10))\n",
    "X_test = np.reshape(X_test,(-1,784))\n",
    "Y_test = np.reshape(Y_test,(-1,10))\n",
    "print(\"Shape of x train\",X_train.shape)\n",
    "print(\"Shape of y train\",Y_train.shape)\n",
    "print(\"Shape of x test\",X_test.shape)\n",
    "print(\"Shape of y test\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Layer Neural Network\n",
    "class TwoLayerNet(object):\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        #initializing parameters\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    # Forward Pass, Backward pass , loss calculation\n",
    "    def loss(self, X_train, Y_train, reg=0.0):\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X_train.shape\n",
    "\n",
    "        # forward pass\n",
    "        scores = None\n",
    "        z1 = X_train.dot(W1) + b1 # Input - Hidden\n",
    "        a1 = np.maximum(0, z1) # ReLU\n",
    "        scores = a1.dot(W2) + b2\n",
    "\n",
    "        # Applying softmax\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) \n",
    "\n",
    "        # Compute the loss\n",
    "        loss = None\n",
    "        correct_logprobs = -np.log(probs[range(N), np.argmax(Y_train)])\n",
    "        data_loss = np.sum(correct_logprobs) / N\n",
    "        reg_loss = 0.5 * reg * np.sum(W1 * W1) + 0.5 * reg * np.sum(W2 * W2)\n",
    "        loss = data_loss + reg_loss\n",
    "\n",
    "        # Backward pass\n",
    "        # compute gradients\n",
    "        grads = {}\n",
    "        dscores = probs\n",
    "        dscores[range(N),np.argmax(Y_train)] -= 1\n",
    "        dscores /= N\n",
    "\n",
    "        # W2 and b2\n",
    "        grads['W2'] = np.dot(a1.T, dscores)\n",
    "        grads['b2'] = np.sum(dscores, axis=0)\n",
    "        \n",
    "        # next backpropagation into hidden layer\n",
    "        dhidden = np.dot(dscores, W2.T)\n",
    "        \n",
    "        # backprop the ReLU non-linearity\n",
    "        dhidden[a1 <= 0] = 0\n",
    "        \n",
    "        # Backpropagation into W1,b1\n",
    "        grads['W1'] = np.dot(X_train.T, dhidden)\n",
    "        grads['b1'] = np.sum(dhidden, axis=0)\n",
    "\n",
    "        # add regularization to gradient\n",
    "        grads['W2'] += reg * W2\n",
    "        grads['W1'] += reg * W1\n",
    "\n",
    "        return loss, grads\n",
    "    \n",
    "    # stochastic gradient Descent update\n",
    "    def sgd(self,X_train,Y_train,l_rate = 0.01):\n",
    "        loss,grads = self.loss(X_train,Y_train)\n",
    "            \n",
    "        # Updating Parameters (weights and biases)\n",
    "        self.params['W1'] = self.params['W1'] - (l_rate * grads['W1'])\n",
    "        self.params['b1'] = self.params['b1'] - (l_rate * grads['b1'])\n",
    "        self.params['W2'] = self.params['W2'] - (l_rate * grads['W2'])\n",
    "        self.params['b2'] = self.params['b2'] - (l_rate * grads['b2'])\n",
    "        return loss\n",
    "    \n",
    "    # Momentum update\n",
    "    def momentum(self,X_train,Y_train,l_rate = 0.01,alpha = 0.75):\n",
    "        loss,grads = self.loss(X_train,Y_train)\n",
    "        \n",
    "        v = {} # velocity\n",
    "        for i in range(2):\n",
    "            v['W'+ str(i+1)] = np.zeros((self.params['W'+str(i+1)].shape[0],self.params['W'+str(i+1)].shape[1]))\n",
    "            v['b'+ str(i+1)] = np.zeros(self.params['b'+str(i+1)].shape[0])\n",
    "        \n",
    "        # momentum update\n",
    "        for i in range(2):\n",
    "            v['W'+str(i+1)] = (alpha * v['W'+str(i+1)]) - (l_rate * grads['W'+str(i+1)])\n",
    "            v['b'+str(i+1)] = (alpha * v['b'+str(i+1)]) - (l_rate * grads['b'+str(i+1)])\n",
    "    \n",
    "        # Updating Parameters (weights and biases)\n",
    "        for i in range(2):\n",
    "            self.params['W'+ str(i+1)] = self.params['W'+ str(i+1)] + v['W'+ str(i+1)]\n",
    "            self.params['b'+ str(i+1)] = self.params['b'+ str(i+1)] + v['b'+ str(i+1)]\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    # Nesterov update\n",
    "    def nesterov(self,X_train,Y_train,l_rate = 0.01,alpha = 0.75):\n",
    "            \n",
    "        v = {} # velocity\n",
    "        for i in range(2):\n",
    "            v['W'+ str(i+1)] = np.zeros((self.params['W'+str(i+1)].shape[0],self.params['W'+str(i+1)].shape[1]))\n",
    "            v['b'+ str(i+1)] = np.zeros(self.params['b'+str(i+1)].shape[0])\n",
    "            \n",
    "        for i in range(2):\n",
    "            self.params['W'+ str(i+1)] = self.params['W'+ str(i+1)] + (alpha * v['W'+ str(i+1)])\n",
    "            self.params['b'+ str(i+1)] = self.params['b'+ str(i+1)] + (alpha * v['b'+ str(i+1)])\n",
    "            \n",
    "        loss,grads = self.loss(X_train,Y_train)\n",
    "        \n",
    "        # nesterov update\n",
    "        for i in range(2):\n",
    "            v['W'+str(i+1)] = (alpha * v['W'+str(i+1)]) - (l_rate * grads['W'+str(i+1)])\n",
    "            v['b'+str(i+1)] = (alpha * v['b'+str(i+1)]) - (l_rate * grads['b'+str(i+1)])\n",
    "    \n",
    "        # Updating Parameters (weights and biases)\n",
    "        for i in range(2):\n",
    "            self.params['W'+ str(i+1)] = self.params['W'+ str(i+1)] + v['W'+ str(i+1)]\n",
    "            self.params['b'+ str(i+1)] = self.params['b'+ str(i+1)] + v['b'+ str(i+1)]\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    # Adagrad update\n",
    "    def adagrad(self,X_train,Y_train,l_rate = 0.01,delta = 1e-7):\n",
    "        \n",
    "        loss,grads = self.loss(X_train,Y_train)\n",
    "        \n",
    "        theta = {} # Delta theta initialization\n",
    "        for i in range(2):\n",
    "            theta['W'+ str(i+1)] = np.zeros((self.params['W'+str(i+1)].shape[0],self.params['W'+str(i+1)].shape[1]))\n",
    "            theta['b'+ str(i+1)] = np.zeros(self.params['b'+str(i+1)].shape[0])\n",
    "        \n",
    "        r = {} # Gradient accumulation variable\n",
    "        # Gradient accumulation variable initialization\n",
    "        for i in range(2):\n",
    "            r['W'+ str(i+1)] = np.zeros((self.params['W'+str(i+1)].shape[0],self.params['W'+str(i+1)].shape[1]))\n",
    "            r['b'+ str(i+1)] = np.zeros(self.params['b'+str(i+1)].shape[0])\n",
    "            \n",
    "        # Accumulate squared gradient\n",
    "        for i in range(2):\n",
    "            r['W'+str(i+1)] += np.multiply(grads['W'+str(i+1)],grads['W'+str(i+1)])\n",
    "            r['b'+str(i+1)] += np.multiply(grads['b'+str(i+1)],grads['b'+str(i+1)])\n",
    "        \n",
    "        # Computing update\n",
    "        for i in range(2):\n",
    "            theta['W'+ str(i+1)] = np.multiply((-l_rate / (delta + np.sqrt(r['W'+str(i+1)]))),grads['W'+str(i+1)])\n",
    "            theta['b'+ str(i+1)] = np.multiply((-l_rate / (delta + np.sqrt(r['b'+str(i+1)]))),grads['b'+str(i+1)])\n",
    "            \n",
    "        # Updating Parameters (weights and biases)\n",
    "        for i in range(2):\n",
    "            self.params['W'+ str(i+1)] = self.params['W'+ str(i+1)] + theta['W'+ str(i+1)]\n",
    "            self.params['b'+ str(i+1)] = self.params['b'+ str(i+1)] + theta['b'+ str(i+1)]\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    # RMSProp update\n",
    "    def RMSProp(self,X_train,Y_train,l_rate = 0.01,delta = 1e-6, decay = 0.9):\n",
    "        \n",
    "        loss,grads = self.loss(X_train,Y_train)\n",
    "        \n",
    "        theta = {} # Delta theta initialization\n",
    "        for i in range(2):\n",
    "            theta['W'+ str(i+1)] = np.zeros((self.params['W'+str(i+1)].shape[0],self.params['W'+str(i+1)].shape[1]))\n",
    "            theta['b'+ str(i+1)] = np.zeros(self.params['b'+str(i+1)].shape[0])\n",
    "        \n",
    "        r = {} # Gradient accumulation variable\n",
    "        # Gradient accumulation variable initialization\n",
    "        for i in range(2):\n",
    "            r['W'+ str(i+1)] = np.zeros((self.params['W'+str(i+1)].shape[0],self.params['W'+str(i+1)].shape[1]))\n",
    "            r['b'+ str(i+1)] = np.zeros(self.params['b'+str(i+1)].shape[0])\n",
    "            \n",
    "        # Accumulate squared gradient\n",
    "        for i in range(2):\n",
    "            r['W'+str(i+1)] = (decay * r['W'+str(i+1)]) + ((1 - decay) * np.multiply(grads['W'+str(i+1)],grads['W'+str(i+1)]))\n",
    "            r['b'+str(i+1)] = (decay * r['b'+str(i+1)]) + ((1 - decay) * np.multiply(grads['b'+str(i+1)],grads['b'+str(i+1)]))\n",
    "        \n",
    "        # Computing update\n",
    "        for i in range(2):\n",
    "            theta['W'+ str(i+1)] = np.multiply(-(l_rate / (delta + np.sqrt(r['W'+str(i+1)]))),grads['W'+str(i+1)])\n",
    "            theta['b'+ str(i+1)] = np.multiply(-(l_rate / (delta + np.sqrt(r['b'+str(i+1)]))),grads['b'+str(i+1)])\n",
    "            \n",
    "        # Updating Parameters (weights and biases)\n",
    "        for i in range(2):\n",
    "            self.params['W'+ str(i+1)] = self.params['W'+ str(i+1)] + theta['W'+ str(i+1)]\n",
    "            self.params['b'+ str(i+1)] = self.params['b'+ str(i+1)] + theta['b'+ str(i+1)]\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    # ADAM update\n",
    "    def ADAM(self,X_train,Y_train,t,l_rate = 0.001,delta = 1e-8, decay1 = 0.9,decay2 = 0.999):\n",
    "        \n",
    "        theta = {} # Delta theta initialization\n",
    "        for i in range(2):\n",
    "            theta['W'+ str(i+1)] = np.zeros((self.params['W'+str(i+1)].shape[0],self.params['W'+str(i+1)].shape[1]))\n",
    "            theta['b'+ str(i+1)] = np.zeros(self.params['b'+str(i+1)].shape[0])\n",
    "        \n",
    "        s = {} # First Moment variable\n",
    "        # First Moment variable initialization\n",
    "        for i in range(2):\n",
    "            s['W'+ str(i+1)] = np.zeros((self.params['W'+str(i+1)].shape[0],self.params['W'+str(i+1)].shape[1]))\n",
    "            s['b'+ str(i+1)] = np.zeros(self.params['b'+str(i+1)].shape[0])\n",
    "        \n",
    "        r = {} # Second Moment variable\n",
    "        # Second Moment variable initialization\n",
    "        for i in range(2):\n",
    "            r['W'+ str(i+1)] = np.zeros((self.params['W'+str(i+1)].shape[0],self.params['W'+str(i+1)].shape[1]))\n",
    "            r['b'+ str(i+1)] = np.zeros(self.params['b'+str(i+1)].shape[0])\n",
    "            \n",
    "        loss,grads = self.loss(X_train,Y_train)\n",
    "        \n",
    "        # First Moment estimate\n",
    "        for i in range(2):\n",
    "            s['W'+str(i+1)] = (decay1 * s['W'+str(i+1)]) + ((1 - decay1) * grads['W'+str(i+1)])\n",
    "            s['b'+str(i+1)] = (decay1 * s['b'+str(i+1)]) + ((1 - decay1) * grads['b'+str(i+1)])\n",
    "            \n",
    "        # Second Moment estimate\n",
    "        for i in range(2):\n",
    "            r['W'+str(i+1)] = (decay2 * r['W'+str(i+1)]) + ((1 - decay2) * np.multiply(grads['W'+str(i+1)],grads['W'+str(i+1)]))\n",
    "            r['b'+str(i+1)] = (decay2 * r['b'+str(i+1)]) + ((1 - decay2) * np.multiply(grads['b'+str(i+1)],grads['b'+str(i+1)]))\n",
    "        \n",
    "        s_hat = {} # Correct Bias First Moment variable\n",
    "        # Correct bias in First Moment variable initialization\n",
    "        for i in range(2):\n",
    "            s_hat['W'+ str(i+1)] = np.zeros((self.params['W'+str(i+1)].shape[0],self.params['W'+str(i+1)].shape[1]))\n",
    "            s_hat['b'+ str(i+1)] = np.zeros(self.params['b'+str(i+1)].shape[0])\n",
    "            \n",
    "        r_hat = {} # Correct Bias in second Moment variable\n",
    "        # Correct Bias in second Moment variable initialization\n",
    "        for i in range(2):\n",
    "            r_hat['W'+ str(i+1)] = np.zeros((self.params['W'+str(i+1)].shape[0],self.params['W'+str(i+1)].shape[1]))\n",
    "            r_hat['b'+ str(i+1)] = np.zeros(self.params['b'+str(i+1)].shape[0])\n",
    "        \n",
    "        # correct bias in first moment\n",
    "        for i in range(2):\n",
    "            s_hat['W'+ str(i+1)] = (s['W'+ str(i+1)] / (1 - (decay1**t)))\n",
    "            s_hat['b'+ str(i+1)] = (s['b'+ str(i+1)] / (1 - (decay2**t)))\n",
    "            \n",
    "        # correct bias in second moment\n",
    "        for i in range(2):\n",
    "            r_hat['W'+ str(i+1)] = (r['W'+ str(i+1)] / (1 - (decay1**t)))\n",
    "            r_hat['b'+ str(i+1)] = (r['b'+ str(i+1)] / (1 - (decay2**t)))\n",
    "            \n",
    "        # Computing update\n",
    "        for i in range(2):\n",
    "            theta['W'+ str(i+1)] = -((l_rate * s_hat['W'+ str(i+1)]) / (delta + np.sqrt(r_hat['W'+str(i+1)])))\n",
    "            theta['b'+ str(i+1)] = -((l_rate * s_hat['b'+ str(i+1)]) / (delta + np.sqrt(r_hat['b'+str(i+1)])))\n",
    "            \n",
    "        # Updating Parameters (weights and biases)\n",
    "        for i in range(2):\n",
    "            self.params['W'+ str(i+1)] = self.params['W'+ str(i+1)] + theta['W'+ str(i+1)]\n",
    "            self.params['b'+ str(i+1)] = self.params['b'+ str(i+1)] + theta['b'+ str(i+1)]\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 1000\n",
    "output_size = 10\n",
    "l_sgd = []\n",
    "NN = TwoLayerNet(input_size,hidden_size,output_size) # Calling Neural Network\n",
    "\n",
    "# Displaying Loss \n",
    "for i in range(100):\n",
    "    loss = NN.sgd(X_train,Y_train)\n",
    "    if i % 10 == 0:\n",
    "        print(\"loss in %d/100 iterations : \" %(i+1),loss)\n",
    "    l_sgd = np.append(l,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting Stochastic Gradient Loss\n",
    "\n",
    "iter = range(100)\n",
    "plt.plot(iter,l,label = 'SGD')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.suptitle('Stochastic Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 1000\n",
    "output_size = 10\n",
    "l_m = []\n",
    "NN = TwoLayerNet(input_size,hidden_size,output_size) # Calling Neural Network\n",
    "\n",
    "# Displaying Loss \n",
    "for i in range(100):\n",
    "    loss = NN.momentum(X_train,Y_train)\n",
    "    if i % 10 == 0:\n",
    "        print(\"loss in %d/100 iterations : \" %(i+1),loss)\n",
    "    l_m = np.append(l_m,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting Stochastic Gradient with momentum Loss\n",
    "\n",
    "iter = range(100)\n",
    "plt.plot(iter,l_m,label = 'SGD with Momentum')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.suptitle('Stochastic Gradient Descent with momentum')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 1000\n",
    "output_size = 10\n",
    "l_n = []\n",
    "NN = TwoLayerNet(input_size,hidden_size,output_size) # Calling Neural Network\n",
    "\n",
    "# Displaying Loss \n",
    "for i in range(100):\n",
    "    loss = NN.nesterov(X_train,Y_train)\n",
    "    if i % 10 == 0:\n",
    "        print(\"loss in %d/100 iterations : \" %(i+1),loss)\n",
    "    l_n = np.append(l_n,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting Nesterov momentum Loss\n",
    "\n",
    "iter = range(100)\n",
    "plt.plot(iter,l_n,label = 'Nesterov momentum')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.suptitle('Nesterov momemtum')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 1000\n",
    "output_size = 10\n",
    "l_g = []\n",
    "NN = TwoLayerNet(input_size,hidden_size,output_size) # Calling Neural Network\n",
    "\n",
    "# Displaying Loss \n",
    "for i in range(100):\n",
    "    loss = NN.adagrad(X_train,Y_train)\n",
    "    if i % 10 == 0:\n",
    "        print(\"loss in %d/100 iterations : \" %(i+1),loss)\n",
    "    l_g = np.append(l_g,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting ADAGrad Loss\n",
    "\n",
    "iter = range(100)\n",
    "plt.plot(iter,l_g,label = 'ADAGrad')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.suptitle('ADAGrad')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 1000\n",
    "output_size = 10\n",
    "l_r = []\n",
    "NN = TwoLayerNet(input_size,hidden_size,output_size) # Calling Neural Network\n",
    "\n",
    "# Displaying Loss \n",
    "for i in range(100):\n",
    "    loss = NN.RMSProp(X_train,Y_train)\n",
    "    if i % 10 == 0:\n",
    "        print(\"loss in %d/100 iterations : \" %(i+1),loss)\n",
    "    l_r = np.append(l_r,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting RMSProp Loss\n",
    "\n",
    "iter = range(100)\n",
    "plt.plot(iter,l_r,label = 'RMSProp')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.suptitle('RMSProp')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 1000\n",
    "output_size = 10\n",
    "l_adam = []\n",
    "NN = TwoLayerNet(input_size,hidden_size,output_size) # Calling Neural Network\n",
    "t = 0\n",
    "# Displaying Loss \n",
    "for i in range(100):\n",
    "    loss = NN.ADAM(X_train,Y_train,t = i+1)\n",
    "    if i % 10 == 0:\n",
    "        print(\"loss in %d/100 iterations : \" %(i+1),loss)\n",
    "    l_adam = np.append(l_adam,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting ADAM Loss\n",
    "\n",
    "iter = range(100)\n",
    "plt.plot(iter,l_adam,label = 'ADAM')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.suptitle('ADAM')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
